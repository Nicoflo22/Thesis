{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import wfdb\n",
    "from scipy import signal\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "import neurokit2 as nk\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.signal import butter, lfilter, freqz, iirnotch\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "pd.reset_option('display.max_columns')  # Show all columns\n",
    "pd.reset_option('display.expand_frame_repr')  # Disable wrapping\n",
    "pd.reset_option('display.max_colwidth')  # Show full contents of each cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "superclass_df = pd.read_csv('superclass.csv')\n",
    "\n",
    "# Initialize empty arrays to store the data\n",
    "data = []\n",
    "\n",
    "# Specify the root directory\n",
    "root_dir = '../ptb-xl/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.3/'\n",
    "\n",
    "# Loop over all records in the DataFrame\n",
    "for idx, row in superclass_df.iterrows():\n",
    "    if (idx+1)%100 == 0:\n",
    "        print(f\"Processing record {idx+1}/{len(superclass_df)}\")\n",
    "    # Get filename without extension\n",
    "    filename = os.path.splitext(row['filename_lr'])[0]\n",
    "\n",
    "    # Construct the full path\n",
    "    full_path = os.path.join(root_dir, filename)\n",
    "\n",
    "    # Load the raw ECG signal data from the .dat file\n",
    "    dat, fields = wfdb.rdsamp(full_path)\n",
    "\n",
    "    # Load the labels data from the .hea file\n",
    "    hea = wfdb.rdheader(full_path)\n",
    "\n",
    "    # Add data to the list\n",
    "    data.append({\n",
    "        'record_name': filename,\n",
    "        'superclass': row['diagnostic_superclass'],\n",
    "        'signal': dat,\n",
    "        'age': row['age'],\n",
    "        'sex': row['sex']\n",
    "    })\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save the DataFrame\n",
    "df.to_pickle('df_processed.pkl')\n",
    "\n",
    "df_orig = pd.read_pickle('df_processed.pkl')\n",
    "df_orig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure each label is in a separate list\n",
    "df['superclass'] = df['superclass'].apply(lambda x: [x] if isinstance(x, str) else x)\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "label_encoded = mlb.fit_transform(df['superclass'])\n",
    "df_encoded = pd.concat([df.drop('superclass', axis=1), pd.DataFrame(label_encoded, columns=mlb.classes_)], axis=1)\n",
    "\n",
    "df_encoded\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column 'DISEASE' which is the sum of the four disease columns\n",
    "df_encoded['DISEASE'] = df_encoded[['[\\'CD\\']', '[\\'HYP\\']', '[\\'MI\\']', '[\\'STTC\\']']].sum(axis=1)\n",
    "\n",
    "# Any row where 'DISEASE' is greater than 0 means the patient has some disease, so set it to 1\n",
    "df_encoded['DISEASE'] = df_encoded['DISEASE'].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# Now you can drop the original four disease columns\n",
    "df_encoded = df_encoded.drop(columns=['[\\'CD\\']', '[\\'HYP\\']', '[\\'MI\\']', '[\\'STTC\\']'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPLIT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Separate the target labels (heart condition classes) and the input features\n",
    "X_ecg = np.array(df_encoded['signal'].tolist())\n",
    "X_info = df_encoded[[]]  # age and sex as additional features\n",
    "y = df_encoded.drop(['record_name', 'signal', 'age', 'sex'], axis=1)\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "X_ecg_train, X_ecg_test, X_info_train, X_info_test, y_train, y_test = train_test_split(\n",
    "    X_ecg, X_info, y, test_size=0.3, random_state=12\n",
    ")\n",
    "X_ecg_val, X_ecg_test, X_info_val, X_info_test, y_val, y_test = train_test_split(\n",
    "    X_ecg_test, X_info_test, y_test, test_size=0.5, random_state=12\n",
    ")\n",
    "\n",
    "# Apply feature scaling (if needed) on the ECG signal data\n",
    "scaler = StandardScaler()\n",
    "X_ecg_train = np.array([scaler.fit_transform(sample) for sample in X_ecg_train])\n",
    "X_ecg_val = np.array([scaler.transform(sample) for sample in X_ecg_val])\n",
    "X_ecg_test = np.array([scaler.transform(sample) for sample in X_ecg_test])\n",
    "\n",
    "# Check the shapes of the datasets\n",
    "print(\"ECG Data Shapes:\")\n",
    "print(\"Training data:\", X_ecg_train.shape)\n",
    "print(\"Validation data:\", X_ecg_val.shape)\n",
    "print(\"Test data:\", X_ecg_test.shape)\n",
    "print(\"\\nInfo Data Shapes:\")\n",
    "print(\"Training data:\", X_info_train.shape)\n",
    "print(\"Validation data:\", X_info_val.shape)\n",
    "print(\"Test data:\", X_info_test.shape)\n",
    "print(\"\\nTarget Labels Shape:\")\n",
    "print(\"Training labels:\", y_train.shape)\n",
    "print(\"Validation labels:\", y_val.shape)\n",
    "print(\"Test labels:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Create a list to store all ECG signals\n",
    "all_ecgs = []\n",
    "\n",
    "# Iterate over the 12 leads\n",
    "for i in range(12):\n",
    "    single_ecg = []\n",
    "    for sublist in df_encoded.loc[3,'signal']:  # Get the data for one patient\n",
    "        single_ecg.append(sublist[i])  # Get data for one lead\n",
    "\n",
    "    signals, info = nk.ecg_process(single_ecg, sampling_rate=100)\n",
    "    cleaned_ecg = signals[\"ECG_Clean\"]\n",
    "\n",
    "    all_ecgs.append(cleaned_ecg)\n",
    "\n",
    "# Combine all ECG signals into a single 2D array\n",
    "all_ecgs = np.stack(all_ecgs)\n",
    "\n",
    "# Plot each ECG lead with a slight vertical shift for better visibility\n",
    "for i, ecg in enumerate(all_ecgs):\n",
    "    plt.plot(ecg + i*2)  # Shift each lead by 200 units\n",
    "\n",
    "plt.title('ECG signals for all 12 leads')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove last beat if incomplete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpt=0\n",
    "def clean_heartbeats(row):\n",
    "    global cpt\n",
    "    all_leads = []\n",
    "    for sublist in row['signal']:\n",
    "        all_leads.append(sublist)  # Get all leads\n",
    "\n",
    "    # Process lead II to get R-peaks\n",
    "    single_ecg_lead_II = [sublist[1] for sublist in row['signal']]  # LEAD II\n",
    "    try:\n",
    "        signals, info = nk.ecg_process(single_ecg_lead_II, sampling_rate=500)\n",
    "    except ValueError as e:\n",
    "        print(f\"An error occurred while processing the ECG data for record {row['record_name']}: {e}\")\n",
    "        return single_ecg_lead_II  # Return the original ECG signal if an error occurs\n",
    "\n",
    "    rpeaks = info[\"ECG_R_Peaks\"]\n",
    "    # Remove NaN values from rpeaks\n",
    "    rpeaks = rpeaks[~np.isnan(rpeaks)]\n",
    "    try:\n",
    "        heartbeats_II = np.split(single_ecg_lead_II, rpeaks.astype(int))\n",
    "    except ValueError as e:\n",
    "        print(f\"An error occurred while splitting the ECG data for record {row['record_name']}: {e}\")\n",
    "        return single_ecg_lead_II  # Return the original ECG signal if an error occurs\n",
    "\n",
    "    # Check if the last heartbeat is incomplete (i.e., significantly shorter than the others)\n",
    "    average_heartbeat_length = np.mean([len(heartbeat) for heartbeat in heartbeats_II])\n",
    "    if len(heartbeats_II[-1]) < average_heartbeat_length * 0.75:\n",
    "        rpeaks = rpeaks[:-1]  # Remove the last R-peak\n",
    "        cpt += 1\n",
    "\n",
    "    # Use the R-peaks to segment all leads and remove incomplete heartbeats\n",
    "    cleaned_ecg_leads = []\n",
    "    for lead in all_leads:\n",
    "        try:\n",
    "            heartbeats = np.split(lead, rpeaks.astype(int))\n",
    "        except ValueError as e:\n",
    "            print(f\"An error occurred while splitting the ECG data for record {row['record_name']}: {e}\")\n",
    "            return lead  # Return the original ECG signal if an error occurs\n",
    "\n",
    "        if len(heartbeats[-1]) < average_heartbeat_length * 0.75:\n",
    "            heartbeats = heartbeats[:-1]  # Remove the last heartbeat if incomplete\n",
    "        # Concatenate the heartbeats back into a single ECG signal\n",
    "        cleaned_ecg_lead = np.concatenate(heartbeats)\n",
    "        cleaned_ecg_leads.append(cleaned_ecg_lead)\n",
    "\n",
    "    # Stack the cleaned leads back together\n",
    "    cleaned_ecg = np.stack(cleaned_ecg_leads, axis=-1)\n",
    "    return cleaned_ecg\n",
    "\n",
    "\n",
    "# Apply the function to the 'signal' column of the DataFrame\n",
    "df_encoded_removed_beats = df_encoded\n",
    "df_encoded_removed_beats['signal'] = df_encoded.apply(clean_heartbeats, axis=1)\n",
    "print(df_encoded_removed_beats)\n",
    "print(df_encoded_removed_beats['signal'][0])\n",
    "print(df_encoded_removed_beats['signal'][0][0])\n",
    "print(df_encoded_removed_beats['signal'][0][0][0])\n",
    "print(df_encoded_removed_beats.shape)\n",
    "print(f\"number of incomplete beats patients: {cpt}/{len(df_encoded.index)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sample rate and the frequencies for the notch and low-pass filters\n",
    "sample_rate = 100\n",
    "notch_freq = 50\n",
    "low_pass_freq = 15  # Adjust the low-pass frequency to maintain similar filtering characteristics\n",
    "\n",
    "def design_filters(sample_rate, notch_freq, low_pass_freq):\n",
    "    # Design the notch filter\n",
    "    nyquist = 0.5 * sample_rate\n",
    "    freq_ratio = notch_freq / nyquist\n",
    "    notch_filter = iirnotch(freq_ratio, 30)  # 30 is the quality factor of the filter\n",
    "\n",
    "    # Design the low-pass filter\n",
    "    low_pass_filter = butter(5, low_pass_freq / nyquist, btype='low')\n",
    "\n",
    "    return notch_filter, low_pass_filter\n",
    "\n",
    "# Design the filters\n",
    "notch_filter, low_pass_filter = design_filters(sample_rate, notch_freq, low_pass_freq)\n",
    "# Define a function to apply the filters\n",
    "def apply_filters(data, notch_filter, low_pass_filter):\n",
    "    # Apply the notch filter\n",
    "    data = lfilter(*notch_filter, data)\n",
    "\n",
    "    # Apply the low-pass filter\n",
    "    data = lfilter(*low_pass_filter, data)\n",
    "\n",
    "    return data\n",
    "# Apply the filters to the data\n",
    "print(df_encoded.shape)\n",
    "df_encoded['signal'] = df_encoded['signal'].apply(lambda signal: apply_filters(signal, notch_filter, low_pass_filter))\n",
    "print(df_encoded.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compute the mean and standard deviation for each record\n",
    "means = np.array([np.mean(record) for record in df_encoded['signal']])\n",
    "std_devs = np.array([np.std(record) for record in df_encoded['signal']])\n",
    "\n",
    "# Compute the average mean and standard deviation\n",
    "avg_mean = np.mean(means)\n",
    "avg_std_dev = np.mean(std_devs)\n",
    "\n",
    "# Compute the standard deviation of the means and standard deviations\n",
    "std_mean = np.std(means)\n",
    "std_std_dev = np.std(std_devs)\n",
    "\n",
    "# Identify outliers as records where the mean or standard deviation is more than 3 standard deviations from the average\n",
    "outlier_indices = np.where(\n",
    "    (np.abs(means - avg_mean) > 3 * std_mean) |\n",
    "    (np.abs(std_devs - avg_std_dev) > 3 * std_std_dev)\n",
    ")[0]\n",
    "\n",
    "# Remove outliers from df\n",
    "df_cleaned = df_encoded.drop(outlier_indices)\n",
    "print(df_cleaned.shape)\n",
    "print(len(df_cleaned['signal'][0]))\n",
    "print(df_cleaned['signal'][0][0])\n",
    "#df_cleaned = df_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Conv1D, BatchNormalization, Activation, Add, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def residual_block(X, filters):\n",
    "    # Save the input value\n",
    "    X_shortcut = X\n",
    "\n",
    "    # First component of main path\n",
    "    X = Conv1D(filters, kernel_size=8, strides=1, padding=\"same\")(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    # Second component of main path\n",
    "    X = Conv1D(filters, kernel_size=5, strides=1, padding=\"same\")(X)\n",
    "    X = BatchNormalization()(X)\n",
    "\n",
    "    # Add shortcut value to main path\n",
    "    X_shortcut = Conv1D(filters, kernel_size=3, strides=1, padding=\"same\")(X_shortcut)\n",
    "    X_shortcut = BatchNormalization()(X_shortcut)\n",
    "\n",
    "    X = Add()([X, X_shortcut])\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    return X\n",
    "\n",
    "def create_model(input_shape=(5000,12), classes=2):\n",
    "    # Define the input\n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "    # Stage 1\n",
    "    X = Conv1D(64, kernel_size=7, strides=2)(X_input)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling1D(pool_size=3, strides=2)(X)\n",
    "\n",
    "    # Stage 2\n",
    "    X = residual_block(X, filters=64)\n",
    "\n",
    "    # Stage 3\n",
    "    X = residual_block(X, filters=128)\n",
    "\n",
    "    # Stage 4\n",
    "    X = residual_block(X, filters=256)\n",
    "\n",
    "    # Output layer\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(classes, activation='sigmoid')(X)\n",
    "\n",
    "    # Create model\n",
    "    model = Model(inputs=X_input, outputs=X, name='ResNet')\n",
    "\n",
    "    return model\n",
    "\n",
    "# Call the function to create the model\n",
    "model_resnet = create_model(input_shape=(5000, 12), classes=1)\n",
    "\n",
    "# Compile the model\n",
    "model_resnet.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model_resnet.summary()\n",
    "\n",
    "history_resnet = model_resnet.fit(\n",
    "    X_ecg_train,\n",
    "    y_train,\n",
    "    validation_data=(X_ecg_val, y_val),\n",
    "    epochs=20,  # specify the number of epochs\n",
    "    batch_size=32,  # specify your batch size\n",
    "    #class_weight = class_weight\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "\n",
    "y_pred = model_resnet.predict(X_ecg_test)  # assuming X_test is your test set\n",
    "threshold = 0.5\n",
    "y_pred_binary = (y_pred > threshold).astype(int)\n",
    "y_test_binary = (y_test > threshold).astype(int)\n",
    "# ROC AUC\n",
    "roc_auc = roc_auc_score(y_test_binary, y_pred_binary)\n",
    "print(f'ROC AUC Score: {roc_auc}')\n",
    "\n",
    "# Classification Report\n",
    "target_names = ['NORM', 'DISEASE']\n",
    "print(classification_report(y_test_binary, y_pred_binary, target_names=target_names))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Assuming you have the true labels in `y_true` and the predicted labels in `y_pred`\n",
    "\n",
    "\n",
    "cm = confusion_matrix(y_test_binary, y_pred_binary)\n",
    "\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_smooth_loss(history, key='loss', window=5):\n",
    "    plt.figure(figsize=(16,10))\n",
    "\n",
    "    val = pd.Series(history.history[key])\n",
    "    val_smooth = val.rolling(window=window, center=True).mean().fillna(val)\n",
    "    plt.plot(val_smooth, label='Validation Loss')\n",
    "\n",
    "    train = pd.Series(history.history[key])\n",
    "    train_smooth = train.rolling(window=window, center=True).mean().fillna(train)\n",
    "    plt.plot(train_smooth, label='Training Loss')\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(key.replace('_',' ').title())\n",
    "    plt.legend()\n",
    "\n",
    "    # Find the maximum epoch for the x-axis limit\n",
    "    max_epoch = max(history.epoch)\n",
    "    plt.xlim([0, max_epoch])\n",
    "\n",
    "    # Set the x-axis ticks to integers\n",
    "    plt.xticks(np.arange(0, max_epoch+1, step=1))\n",
    "    plt.title('Loss per Epoch')\n",
    "\n",
    "# Plot the loss\n",
    "plot_smooth_loss(history_resnet)\n",
    "\n",
    "def plot_smooth_accuracy(history, key='accuracy', window=5):\n",
    "    plt.figure(figsize=(16,10))\n",
    "\n",
    "    val = pd.Series(history.history['val_'+key])\n",
    "    val_smooth = val.rolling(window=window, center=True).mean().fillna(val)\n",
    "    plt.plot(val_smooth, label='Validation Accuracy')\n",
    "\n",
    "    train = pd.Series(history.history[key])\n",
    "    train_smooth = train.rolling(window=window, center=True).mean().fillna(train)\n",
    "    plt.plot(train_smooth, label='Training Accuracy')\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(key.replace('_',' ').title())\n",
    "    plt.legend()\n",
    "\n",
    "    # Find the maximum epoch for the x-axis limit\n",
    "    max_epoch = max(history.epoch)\n",
    "    plt.xlim([0, max_epoch])\n",
    "\n",
    "    # Set the x-axis ticks to integers\n",
    "    plt.xticks(np.arange(0, max_epoch+1, step=1))\n",
    "    plt.title('Accuracy per Epoch')\n",
    "\n",
    "# Plot the accuracy\n",
    "plot_smooth_accuracy(history_resnet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Normalization and Attention\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(x, x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    return x + res\n",
    "\n",
    "inputs = layers.Input(shape=(5000, 12))\n",
    "x = transformer_encoder(inputs, head_size=64, num_heads=2, ff_dim=512, dropout=0.1)\n",
    "\n",
    "# The Global Average Pooling layer and output layer\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model_tran = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model_tran.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n",
    "\n",
    "# Pass it as a callback to the fit function\n",
    "history_tran = model_tran.fit(X_ecg_train, y_train, batch_size=32, epochs=100, validation_data=(X_ecg_val, y_val), callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_smooth_loss(history, key='loss', window=5):\n",
    "    plt.figure(figsize=(16,10))\n",
    "\n",
    "    val = pd.Series(history.history[key])\n",
    "    val_smooth = val.rolling(window=window, center=True).mean().fillna(val)\n",
    "    plt.plot(val_smooth, label='Validation Loss')\n",
    "\n",
    "    train = pd.Series(history.history[key])\n",
    "    train_smooth = train.rolling(window=window, center=True).mean().fillna(train)\n",
    "    plt.plot(train_smooth, label='Training Loss')\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(key.replace('_',' ').title())\n",
    "    plt.legend()\n",
    "\n",
    "    # Find the maximum epoch for the x-axis limit\n",
    "    max_epoch = max(history.epoch)\n",
    "    plt.xlim([0, max_epoch])\n",
    "\n",
    "    # Set the x-axis ticks to integers\n",
    "    plt.xticks(np.arange(0, max_epoch+1, step=1))\n",
    "    plt.title('Loss per Epoch')\n",
    "\n",
    "# Plot the loss\n",
    "plot_smooth_loss(history_tran)\n",
    "\n",
    "def plot_smooth_accuracy(history, key='accuracy', window=5):\n",
    "    plt.figure(figsize=(16,10))\n",
    "\n",
    "    val = pd.Series(history.history['val_'+key])\n",
    "    val_smooth = val.rolling(window=window, center=True).mean().fillna(val)\n",
    "    plt.plot(val_smooth, label='Validation Accuracy')\n",
    "\n",
    "    train = pd.Series(history.history[key])\n",
    "    train_smooth = train.rolling(window=window, center=True).mean().fillna(train)\n",
    "    plt.plot(train_smooth, label='Training Accuracy')\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(key.replace('_',' ').title())\n",
    "    plt.legend()\n",
    "\n",
    "    # Find the maximum epoch for the x-axis limit\n",
    "    max_epoch = max(history.epoch)\n",
    "    plt.xlim([0, max_epoch])\n",
    "\n",
    "    # Set the x-axis ticks to integers\n",
    "    plt.xticks(np.arange(0, max_epoch+1, step=1))\n",
    "    plt.title('Accuracy per Epoch')\n",
    "\n",
    "# Plot the accuracy\n",
    "plot_smooth_accuracy(history_tran)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Assuming you have the true labels in `y_true` and the predicted labels in `y_pred`\n",
    "\n",
    "\n",
    "cm = confusion_matrix(y_test_binary, y_pred_binary)\n",
    "\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def create_model(input_shape=(5000, 12), num_classes=1):\n",
    "    # Define an input for the series\n",
    "    input_series = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Apply LSTM to the series\n",
    "    x = layers.LSTM(64)(input_series)\n",
    "\n",
    "    # Add a dense layer\n",
    "    x = layers.Dense(64, activation=\"relu\")(x)\n",
    "\n",
    "    # Add a dropout layer for regularization\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "\n",
    "    # Add the final classification (output) layer\n",
    "    outputs = layers.Dense(num_classes, activation=\"sigmoid\")(x)\n",
    "\n",
    "    # Define the model\n",
    "    model = Model(inputs=input_series, outputs=outputs)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Call the function to create the model\n",
    "model_rnn = create_model(input_shape=(5000, 12), num_classes=1)\n",
    "\n",
    "# Print the model summary\n",
    "model_rnn.summary()\n",
    "\n",
    "# Then pass the data to the fit method\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n",
    "\n",
    "# Pass it as a callback to the fit function\n",
    "history_rnn = model_rnn.fit(X_ecg_train, y_train, batch_size=32, epochs=100, validation_data=(X_ecg_val, y_val), callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_smooth_loss(history, key='loss', window=5):\n",
    "    plt.figure(figsize=(16,10))\n",
    "\n",
    "    val = pd.Series(history.history['val_'+key])\n",
    "    val_smooth = val.rolling(window=window, center=True).mean().fillna(val)\n",
    "    plt.plot(val_smooth, label='Validation Loss')\n",
    "\n",
    "    train = pd.Series(history.history[key])\n",
    "    train_smooth = train.rolling(window=window, center=True).mean().fillna(train)\n",
    "    plt.plot(train_smooth, label='Training Loss')\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(key.replace('_',' ').title())\n",
    "    plt.legend()\n",
    "\n",
    "    # Find the maximum epoch for the x-axis limit\n",
    "    max_epoch = max(history.epoch)\n",
    "    plt.xlim([0, max_epoch])\n",
    "\n",
    "    # Set the x-axis ticks to integers\n",
    "    plt.xticks(np.arange(0, max_epoch+1, step=1))\n",
    "    plt.title('Loss per Epoch')\n",
    "\n",
    "# Plot the loss\n",
    "plot_smooth_loss(history_rnn)\n",
    "\n",
    "def plot_smooth_accuracy(history, key='accuracy', window=5):\n",
    "    plt.figure(figsize=(16,10))\n",
    "\n",
    "    val = pd.Series(history.history['val_'+key])\n",
    "    val_smooth = val.rolling(window=window, center=True).mean().fillna(val)\n",
    "    plt.plot(val_smooth, label='Validation Accuracy')\n",
    "\n",
    "    train = pd.Series(history.history[key])\n",
    "    train_smooth = train.rolling(window=window, center=True).mean().fillna(train)\n",
    "    plt.plot(train_smooth, label='Training Accuracy')\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(key.replace('_',' ').title())\n",
    "    plt.legend()\n",
    "\n",
    "    # Find the maximum epoch for the x-axis limit\n",
    "    max_epoch = max(history.epoch)\n",
    "    plt.xlim([0, max_epoch])\n",
    "\n",
    "    # Set the x-axis ticks to integers\n",
    "    plt.xticks(np.arange(0, max_epoch+1, step=1))\n",
    "    plt.title('Accuracy per Epoch')\n",
    "\n",
    "# Plot the accuracy\n",
    "plot_smooth_accuracy(history_rnn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10-Fold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = df_encoded['signal'].to_numpy()\n",
    "X = np.array([np.array(x) for x in df_encoded['signal']])\n",
    "\n",
    "y = df_encoded.drop(['record_name', 'signal', 'age', 'sex'], axis=1)\n",
    "\n",
    "n_folds = 10\n",
    "kfold = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "scores = []\n",
    "\n",
    "for train_index, val_index in kfold.split(X):\n",
    "    X_train, X_val = X[train_index], X[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    # Define the ECG branch of the model\n",
    "    ecg_input = layers.Input(shape=(5000, 12))\n",
    "    ecg_layer = layers.Conv1D(64, 7, activation='relu')(ecg_input)\n",
    "    ecg_layer = layers.MaxPooling1D(3)(ecg_layer)\n",
    "    ecg_layer = layers.Conv1D(64, 7, activation='relu')(ecg_layer)\n",
    "    ecg_layer = layers.MaxPooling1D(3)(ecg_layer)\n",
    "    ecg_layer = layers.Dropout(0.5)(ecg_layer)\n",
    "    ecg_layer = layers.Flatten()(ecg_layer)\n",
    "\n",
    "\n",
    "    # Add a couple of Dense layers\n",
    "    output = layers.Dense(64, activation='relu')(ecg_layer)\n",
    "    output = layers.Dropout(0.5)(output)\n",
    "    output = layers.Dense(y_train.shape[1], activation='sigmoid')(output)\n",
    "\n",
    "    model_cnn = tf.keras.Model(inputs=[ecg_input], outputs=output)\n",
    "    model_cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model_cnn.fit(X_train, y_train, epochs=10)\n",
    "    score = model_cnn.evaluate(X_val, y_val)  # Set verbose=0 to not print evaluation results\n",
    "    scores.append(score)\n",
    "\n",
    "scores = np.array(scores)\n",
    "mean_score = np.mean(scores[:, 1])\n",
    "std_score = np.std(scores[:, 1])\n",
    "\n",
    "# Compute the 95% confidence interval\n",
    "confidence_interval = 1.96 * (std_score / np.sqrt(n_folds))\n",
    "lower_bound = mean_score - confidence_interval\n",
    "upper_bound = mean_score + confidence_interval\n",
    "\n",
    "print(f'Mean validation accuracy over {n_folds}-fold cross-validation: {mean_score * 100:.2f}%')\n",
    "print(f'Standard deviation: {std_score * 100:.2f}%')\n",
    "print(f'95% confidence interval: ({lower_bound * 100:.2f}%, {upper_bound * 100:.2f}%)')\n",
    "\n",
    "plt.hist(scores[:, 1], bins=10)\n",
    "plt.title(f'Histogram of validation accuracy over {n_folds}-fold cross-validation')\n",
    "plt.xlabel('Validation Accuracy')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(scores[:, 1])\n",
    "plt.title('Box plot of accuracies over 10-fold cross-validation')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(x=[1], y=[mean_score], yerr=[confidence_interval], fmt='o', color='blue', ecolor='red', capsize=5)\n",
    "plt.title('Mean accuracy with 95% confidence interval')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks([])\n",
    "plt.show()\n",
    "print(confidence_interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 leads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned_8 = df_encoded\n",
    "# Extract only the first eight leads from every ECG signal\n",
    "df_cleaned_8['signal'] = df_cleaned_4['signal'].apply(lambda signal: signal[:, :8])\n",
    "\n",
    "X_8 = df_cleaned_8['signal'].to_numpy()\n",
    "X_8 = np.array([np.array(x) for x in df_cleaned_8['signal']])\n",
    "\n",
    "y_8 = df_cleaned_8.drop(['record_name', 'signal', 'age', 'sex'], axis=1)\n",
    "\n",
    "n_folds_8 = 10\n",
    "kfold_8 = KFold(n_splits=n_folds_8, shuffle=True, random_state=42)\n",
    "scores_8 = []\n",
    "\n",
    "for train_index, val_index in kfold_8.split(X_8):\n",
    "    X_train_8, X_val_8 = X_8[train_index], X_8[val_index]\n",
    "    y_train_8, y_val_8 = y_8.iloc[train_index], y_8.iloc[val_index]\n",
    "\n",
    "    # Define the ECG branch of the model\n",
    "    ecg_input_8 = layers.Input(shape=(5000, 8))  # Adjust to match the number of leads\n",
    "    ecg_layer_8 = layers.Conv1D(64, 7, activation='relu')(ecg_input_8)\n",
    "    ecg_layer_8 = layers.MaxPooling1D(3)(ecg_layer_8)\n",
    "    ecg_layer_8 = layers.Conv1D(64, 7, activation='relu')(ecg_layer_8)\n",
    "    ecg_layer_8 = layers.MaxPooling1D(3)(ecg_layer_8)\n",
    "    ecg_layer_8 = layers.Dropout(0.5)(ecg_layer_8)\n",
    "    ecg_layer_8 = layers.Flatten()(ecg_layer_8)\n",
    "\n",
    "    # Add a couple of Dense layers\n",
    "    output_8 = layers.Dense(64, activation='relu')(ecg_layer_8)\n",
    "    output_8 = layers.Dropout(0.5)(output_8)\n",
    "    output_8 = layers.Dense(y_train_8.shape[1], activation='sigmoid')(output_8)\n",
    "\n",
    "    model_cnn_8 = tf.keras.Model(inputs=[ecg_input_8], outputs=output_8)\n",
    "    model_cnn_8.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model_cnn_8.fit(X_train_8, y_train_8, epochs=10)\n",
    "    score_8 = model_cnn_8.evaluate(X_val_8, y_val_8, verbose=0)  # Set verbose=0 to not print evaluation results\n",
    "    scores_8.append(score_8)\n",
    "\n",
    "scores_8 = np.array(scores_8)\n",
    "mean_score_8 = np.mean(scores_8[:, 1])\n",
    "std_score_8 = np.std(scores_8[:, 1])\n",
    "\n",
    "# Compute the 95% confidence interval\n",
    "confidence_interval_8 = 1.96 * (std_score_8 / np.sqrt(n_folds_8))\n",
    "lower_bound_8 = mean_score_8 - confidence_interval_8\n",
    "upper_bound_8 = mean_score_8 + confidence_interval_8\n",
    "\n",
    "print(f'Mean validation accuracy over {n_folds_8}-fold cross-validation: {mean_score_8 * 100:.2f}%')\n",
    "print(f'Standard deviation: {std_score_8 * 100:.2f}%')\n",
    "print(f'95% confidence interval: ({lower_bound_8 * 100:.2f}%, {upper_bound_8 * 100:.2f}%)')\n",
    "\n",
    "plt.hist(scores_8[:, 1], bins=10)\n",
    "plt.title(f'Histogram of validation accuracy over {n_folds_8}-fold cross-validation')\n",
    "plt.xlabel('Validation Accuracy')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(scores_8[:, 1])\n",
    "plt.title('Box plot of accuracies over 10-fold cross-validation')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(x=[1], y=[mean_score_8], yerr=[confidence_interval_8], fmt='o', color='blue', ecolor='red', capsize=5)\n",
    "plt.title('Mean accuracy with 95% confidence interval')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks([])\n",
    "plt.show()\n",
    "print(confidence_interval_8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 leads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned_4 = df_encoded\n",
    "# Extract only the first four leads from every ECG signal\n",
    "df_cleaned_4['signal'] = df_cleaned_4['signal'].apply(lambda signal: signal[:, :4])\n",
    "\n",
    "X_4 = df_cleaned_4['signal'].to_numpy()\n",
    "X_4 = np.array([np.array(x) for x in df_cleaned_4['signal']])\n",
    "\n",
    "y_4 = df_cleaned_4.drop(['record_name', 'signal', 'age', 'sex'], axis=1)\n",
    "\n",
    "n_folds_4 = 10\n",
    "kfold_4 = KFold(n_splits=n_folds_4, shuffle=True, random_state=42)\n",
    "scores_4 = []\n",
    "\n",
    "for train_index, val_index in kfold_4.split(X_4):\n",
    "    X_train_4, X_val_4 = X_4[train_index], X_4[val_index]\n",
    "    y_train_4, y_val_4 = y_4.iloc[train_index], y_4.iloc[val_index]\n",
    "\n",
    "    # Define the ECG branch of the model\n",
    "    ecg_input_4 = layers.Input(shape=(5000, 4))  # Adjust to match the number of leads\n",
    "    ecg_layer_4 = layers.Conv1D(64, 7, activation='relu')(ecg_input_4)\n",
    "    ecg_layer_4 = layers.MaxPooling1D(3)(ecg_layer_4)\n",
    "    ecg_layer_4 = layers.Conv1D(64, 7, activation='relu')(ecg_layer_4)\n",
    "    ecg_layer_4 = layers.MaxPooling1D(3)(ecg_layer_4)\n",
    "    ecg_layer_4 = layers.Dropout(0.5)(ecg_layer_4)\n",
    "    ecg_layer_4 = layers.Flatten()(ecg_layer_4)\n",
    "\n",
    "    # Add a couple of Dense layers\n",
    "    output_4 = layers.Dense(64, activation='relu')(ecg_layer_4)\n",
    "    output_4 = layers.Dropout(0.5)(output_4)\n",
    "    output_4 = layers.Dense(y_train_4.shape[1], activation='sigmoid')(output_4)\n",
    "\n",
    "    model_cnn_4 = tf.keras.Model(inputs=[ecg_input_4], outputs=output_4)\n",
    "    model_cnn_4.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model_cnn_4.fit(X_train_4, y_train_4, epochs=10)\n",
    "    score_4 = model_cnn_4.evaluate(X_val_4, y_val_4, verbose=0)  # Set verbose=0 to not print evaluation results\n",
    "    scores_4.append(score_4)\n",
    "\n",
    "scores_4 = np.array(scores_4)\n",
    "mean_score_4 = np.mean(scores_4[:, 1])\n",
    "std_score_4 = np.std(scores_4[:, 1])\n",
    "\n",
    "# Compute the 95% confidence interval\n",
    "confidence_interval_4 = 1.96 * (std_score_4 / np.sqrt(n_folds_4))\n",
    "lower_bound_4 = mean_score_4 - confidence_interval_4\n",
    "upper_bound_4 = mean_score_4 + confidence_interval_4\n",
    "\n",
    "print(f'Mean validation accuracy over {n_folds_4}-fold cross-validation: {mean_score_4 * 100:.2f}%')\n",
    "print(f'Standard deviation: {std_score_4 * 100:.2f}%')\n",
    "print(f'95% confidence interval: ({lower_bound_4 * 100:.2f}%, {upper_bound_4 * 100:.2f}%)')\n",
    "\n",
    "plt.hist(scores_4[:, 1], bins=10)\n",
    "plt.title(f'Histogram of validation accuracy over {n_folds_4}-fold cross-validation')\n",
    "plt.xlabel('Validation Accuracy')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(scores_4[:, 1])\n",
    "plt.title('Box plot of accuracies over 10-fold cross-validation')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(x=[1], y=[mean_score_4], yerr=[confidence_interval_4], fmt='o', color='blue', ecolor='red', capsize=5)\n",
    "plt.title('Mean accuracy with 95% confidence interval')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks([])\n",
    "plt.show()\n",
    "print(confidence_interval_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 lead (II)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned_2 = df_encoded\n",
    "# Extract only the first two leads from every ECG signal\n",
    "df_cleaned_2['signal'] = df_cleaned_2['signal'].apply(lambda signal: signal[:, :2])\n",
    "\n",
    "X_2 = df_cleaned_2['signal'].to_numpy()\n",
    "X_2 = np.array([np.array(x) for x in df_cleaned_2['signal']])\n",
    "\n",
    "y_2 = df_cleaned_2.drop(['record_name', 'signal', 'age', 'sex'], axis=1)\n",
    "\n",
    "n_folds_2 = 10\n",
    "kfold_2 = KFold(n_splits=n_folds_2, shuffle=True, random_state=42)\n",
    "scores_2 = []\n",
    "\n",
    "for train_index, val_index in kfold_2.split(X_2):\n",
    "    X_train_2, X_val_2 = X_2[train_index], X_2[val_index]\n",
    "    y_train_2, y_val_2 = y_2.iloc[train_index], y_2.iloc[val_index]\n",
    "\n",
    "    # Define the ECG branch of the model\n",
    "    ecg_input_2 = layers.Input(shape=(5000, 2))\n",
    "    ecg_layer_2 = layers.Conv1D(64, 7, activation='relu')(ecg_input_2)\n",
    "    ecg_layer_2 = layers.MaxPooling1D(3)(ecg_layer_2)\n",
    "    ecg_layer_2 = layers.Conv1D(64, 7, activation='relu')(ecg_layer_2)\n",
    "    ecg_layer_2 = layers.MaxPooling1D(3)(ecg_layer_2)\n",
    "    ecg_layer_2 = layers.Dropout(0.5)(ecg_layer_2)\n",
    "    ecg_layer_2 = layers.Flatten()(ecg_layer_2)\n",
    "\n",
    "    # Add a couple of Dense layers\n",
    "    output_2 = layers.Dense(64, activation='relu')(ecg_layer_2)\n",
    "    output_2 = layers.Dropout(0.5)(output_2)\n",
    "    output_2 = layers.Dense(y_train_2.shape[1], activation='sigmoid')(output_2)\n",
    "\n",
    "    model_cnn_2 = tf.keras.Model(inputs=[ecg_input_2], outputs=output_2)\n",
    "    model_cnn_2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model_cnn_2.fit(X_train_2, y_train_2, epochs=10)\n",
    "    score_2 = model_cnn_2.evaluate(X_val_2, y_val_2, verbose=0)\n",
    "    scores_2.append(score_2)\n",
    "\n",
    "scores_2 = np.array(scores_2)\n",
    "mean_score_2 = np.mean(scores_2[:, 1])\n",
    "std_score_2 = np.std(scores_2[:, 1])\n",
    "\n",
    "# Compute the 95% confidence interval\n",
    "confidence_interval_2 = 1.96 * (std_score_2 / np.sqrt(n_folds_2))\n",
    "lower_bound_2 = mean_score_2 - confidence_interval_2\n",
    "upper_bound_2 = mean_score_2 + confidence_interval_2\n",
    "\n",
    "print(f'Mean validation accuracy over {n_folds_2}-fold cross-validation: {mean_score_2 * 100:.2f}%')\n",
    "print(f'Standard deviation: {std_score_2 * 100:.2f}%')\n",
    "print(f'95% confidence interval: ({lower_bound_2 * 100:.2f}%, {upper_bound_2 * 100:.2f}%)')\n",
    "\n",
    "plt.hist(scores_2[:, 1], bins=10)\n",
    "plt.title(f'Histogram of validation accuracy over {n_folds_2}-fold cross-validation')\n",
    "plt.xlabel('Validation Accuracy')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned_2 = df_encoded\n",
    "# Extract only the first two leads from every ECG signal\n",
    "df_cleaned_2['signal'] = df_cleaned_2['signal'].apply(lambda signal: signal[:, :2])\n",
    "\n",
    "X_2 = df_cleaned_2['signal'].to_numpy()\n",
    "X_2 = np.array([np.array(x) for x in df_cleaned_2['signal']])\n",
    "\n",
    "y_2 = df_cleaned_2.drop(['record_name', 'signal', 'age', 'sex'], axis=1)\n",
    "\n",
    "n_folds_2 = 10\n",
    "kfold_2 = KFold(n_splits=n_folds_2, shuffle=True, random_state=42)\n",
    "scores_2 = []\n",
    "\n",
    "for train_index, val_index in kfold_2.split(X_2):\n",
    "    X_train_2, X_val_2 = X_2[train_index], X_2[val_index]\n",
    "    y_train_2, y_val_2 = y_2.iloc[train_index], y_2.iloc[val_index]\n",
    "\n",
    "    # Define the ECG branch of the model\n",
    "    ecg_input_2 = layers.Input(shape=(5000, 2))\n",
    "    ecg_layer_2 = layers.Conv1D(64, 7, activation='relu')(ecg_input_2)\n",
    "    ecg_layer_2 = layers.MaxPooling1D(3)(ecg_layer_2)\n",
    "    ecg_layer_2 = layers.Conv1D(64, 7, activation='relu')(ecg_layer_2)\n",
    "    ecg_layer_2 = layers.MaxPooling1D(3)(ecg_layer_2)\n",
    "    ecg_layer_2 = layers.Dropout(0.5)(ecg_layer_2)\n",
    "    ecg_layer_2 = layers.Flatten()(ecg_layer_2)\n",
    "\n",
    "    # Add a couple of Dense layers\n",
    "    output_2 = layers.Dense(64, activation='relu')(ecg_layer_2)\n",
    "    output_2 = layers.Dropout(0.5)(output_2)\n",
    "    output_2 = layers.Dense(y_train_2.shape[1], activation='sigmoid')(output_2)\n",
    "\n",
    "    model_cnn_2 = tf.keras.Model(inputs=[ecg_input_2], outputs=output_2)\n",
    "    model_cnn_2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model_cnn_2.fit(X_train_2, y_train_2, epochs=10)\n",
    "    score_2 = model_cnn_2.evaluate(X_val_2, y_val_2, verbose=0)\n",
    "    scores_2.append(score_2)\n",
    "\n",
    "scores_2 = np.array(scores_2)\n",
    "mean_score_2 = np.mean(scores_2[:, 1])\n",
    "std_score_2 = np.std(scores_2[:, 1])\n",
    "\n",
    "# Compute the 95% confidence interval\n",
    "confidence_interval_2 = 1.96 * (std_score_2 / np.sqrt(n_folds_2))\n",
    "lower_bound_2 = mean_score_2 - confidence_interval_2\n",
    "upper_bound_2 = mean_score_2 + confidence_interval_2\n",
    "\n",
    "print(f'Mean validation accuracy over {n_folds_2}-fold cross-validation: {mean_score_2 * 100:.2f}%')\n",
    "print(f'Standard deviation: {std_score_2 * 100:.2f}%')\n",
    "print(f'95% confidence interval: ({lower_bound_2 * 100:.2f}%, {upper_bound_2 * 100:.2f}%)')\n",
    "\n",
    "plt.hist(scores_2[:, 1], bins=10)\n",
    "plt.title(f'Histogram of validation accuracy over {n_folds_2}-fold cross-validation')\n",
    "plt.xlabel('Validation Accuracy')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(x=[1], y=[mean_score_2], yerr=[confidence_interval_2], fmt='o', color='blue', ecolor='red', capsize=5)\n",
    "plt.title('Mean accuracy with 95% confidence interval')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks([])\n",
    "plt.show()\n",
    "print(confidence_interval_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(scores_2[:, 1])\n",
    "plt.title('Box plot of accuracies over 10-fold cross-validation')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 lead (II)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned_1 = df_encoded\n",
    "# Extract only the first lead from every ECG signal\n",
    "df_cleaned_1['signal'] = df_cleaned_1['signal'].apply(lambda signal: signal[:, :1])\n",
    "\n",
    "X_1 = df_cleaned_1['signal'].to_numpy()\n",
    "X_1 = np.array([np.array(x) for x in df_cleaned_1['signal']])\n",
    "\n",
    "y_1 = df_cleaned_1.drop(['record_name', 'signal', 'age', 'sex'], axis=1)\n",
    "\n",
    "n_folds_1 = 10\n",
    "kfold_1 = KFold(n_splits=n_folds_1, shuffle=True, random_state=42)\n",
    "scores_1 = []\n",
    "\n",
    "for train_index, val_index in kfold_1.split(X_1):\n",
    "    X_train_1, X_val_1 = X_1[train_index], X_1[val_index]\n",
    "    y_train_1, y_val_1 = y_1.iloc[train_index], y_1.iloc[val_index]\n",
    "\n",
    "    # Define the ECG branch of the model\n",
    "    ecg_input_1 = layers.Input(shape=(5000, 1))\n",
    "    ecg_layer_1 = layers.Conv1D(64, 7, activation='relu')(ecg_input_1)\n",
    "    ecg_layer_1 = layers.MaxPooling1D(3)(ecg_layer_1)\n",
    "    ecg_layer_1 = layers.Conv1D(64, 7, activation='relu')(ecg_layer_1)\n",
    "    ecg_layer_1 = layers.MaxPooling1D(3)(ecg_layer_1)\n",
    "    ecg_layer_1 = layers.Dropout(0.5)(ecg_layer_1)\n",
    "    ecg_layer_1 = layers.Flatten()(ecg_layer_1)\n",
    "\n",
    "    # Add a couple of Dense layers\n",
    "    output_1 = layers.Dense(64, activation='relu')(ecg_layer_1)\n",
    "    output_1 = layers.Dropout(0.5)(output_1)\n",
    "    output_1 = layers.Dense(y_train_1.shape[1], activation='sigmoid')(output_1)\n",
    "\n",
    "    model_cnn_1 = tf.keras.Model(inputs=[ecg_input_1], outputs=output_1)\n",
    "    model_cnn_1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model_cnn_1.fit(X_train_1, y_train_1, epochs=10)\n",
    "    score_1 = model_cnn_1.evaluate(X_val_1, y_val_1, verbose=0)\n",
    "    scores_1.append(score_1)\n",
    "\n",
    "scores_1 = np.array(scores_1)\n",
    "mean_score_1 = np.mean(scores_1[:, 1])\n",
    "std_score_1 = np.std(scores_1[:, 1])\n",
    "\n",
    "# Compute the 95% confidence interval\n",
    "confidence_interval_1 = 1.96 * (std_score_1 / np.sqrt(n_folds_1))\n",
    "lower_bound_1 = mean_score_1 - confidence_interval_1\n",
    "upper_bound_1 = mean_score_1 + confidence_interval_1\n",
    "\n",
    "print(f'Mean validation accuracy over {n_folds_1}-fold cross-validation: {mean_score_1 * 100:.2f}%')\n",
    "print(f'Standard deviation: {std_score_1 * 100:.2f}%')\n",
    "print(f'95% confidence interval: ({lower_bound_1 * 100:.2f}%, {upper_bound_1 * 100:.2f}%)')\n",
    "\n",
    "plt.hist(scores_1[:, 1], bins=10)\n",
    "plt.title(f'Histogram of validation accuracy over {n_folds_1}-fold cross-validation')\n",
    "plt.xlabel('Validation Accuracy')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(x=[1], y=[mean_score_1], yerr=[confidence_interval_1], fmt='o', color='blue', ecolor='red', capsize=5)\n",
    "plt.title('Mean accuracy with 95% confidence interval')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks([])\n",
    "plt.show()\n",
    "print(confidence_interval_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(scores_1[:, 1])\n",
    "plt.title('Box plot of accuracies over 10-fold cross-validation')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extracting just the accuracies (assuming they are the second value in each list)\n",
    "accuracies_1 = [s[1] for s in scores_1]\n",
    "accuracies_2 = [s[1] for s in scores_2]\n",
    "accuracies_4 = [s[1] for s in scores_4]\n",
    "accuracies_8 = [s[1] for s in scores_8]\n",
    "accuracies_12 = [s[1] for s in scores]\n",
    "\n",
    "data = {\n",
    "    '1-lead': accuracies_1,\n",
    "    '2-lead': accuracies_2,\n",
    "    '4-lead': accuracies_4,\n",
    "    '8-lead': accuracies_8,\n",
    "    '12-lead': accuracies_12\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a format suitable for seaborn\n",
    "labels, values = [], []\n",
    "for k, v in data.items():\n",
    "    labels.extend([k] * len(v))\n",
    "    values.extend(v)\n",
    "\n",
    "# Create a DataFrame\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({'Lead Configuration': labels, 'Accuracy': values})\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='Lead Configuration', y='Accuracy', data=df, order=['1-lead', '2-lead', '4-lead', '8-lead', '12-lead'])\n",
    "plt.title('Boxplots of Accuracies for Each Lead Configuration')\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
